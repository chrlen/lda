% Encoding: UTF-8

@Misc{Blei2003,
 author        = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I. and Lafferty, John},
 title         = {Latent dirichlet allocation},
 year          = {2003},
 __markedentry = {[me:]},
 abstract      = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.},
}

@Misc{Heinrich2005,
 author        = {Heinrich},
 title         = {Parameter estimation for text analysis},
 year          = {2005},
}

@Misc{PhilipResnik2009,
 author        = {Philip Resnik, Eric Hardisty},
 title         = {Gibbs Sampling for the Uninitiated},
 year          = {2009},
 __markedentry = {[me:6]},
 abstract      = {VERSION 0.3 This document is intended for computer scientists who would like to try out a Markov Chain Monte Carlo (MCMC) technique, particularly in order to do inference with Bayesian models on problems related to text processing. We try to keep theory to the absolute minimum needed, and we work through the details much more explicitly than you usually see even in “introductory ” explanations. That means we’ve attempted to be ridiculously explicit in our exposition and notation. After providing the reasons and reasoning behind Gibbs sampling (and at least nodding our heads in the direction of theory), we work through two applications in detail. The first is the derivation of a Gibbs sampler for Naive Bayes models, which illustrates a simple case where the math works out very cleanly and it’s possible to “integrate out ” the model’s continuous parameters to build a more efficient algorithm. The second application derives the Gibbs sampler for a model that is similar to Naive Bayes, but which adds an additional latent variable. Having gone through the two examples, we discuss some practical implementation issues. We conclude with some pointers to literature that we’ve found to be somewhat more friendly to uninitiated readers. 1},
}

@misc{simplewi84:online,
 author = {},
 title = {simplewiki dump progress on 20181120},
 howpublished = {\url{https://dumps.wikimedia.org/simplewiki/20181120/}},
 month = {},
 year = {},
 note = {(Accessed on 12/15/2018)}
}

@misc{gensimTo76:online,
 author = {},
 title = {gensim: Topic modelling for humans},
 howpublished = {\url{https://radimrehurek.com/gensim/}},
 month = {},
 year = {},
 note = {(Accessed on 12/15/2018)}
}

@misc{GitHubbm62:online,
 author = {},
 title = {GitHub - bmabey/pyLDAvis: Python library for interactive topic model visualization. Port of the R LDAvis package.},
 howpublished = {\url{https://github.com/bmabey/pyLDAvis}},
 month = {},
 year = {},
 note = {(Accessed on 12/15/2018)}
}

@misc{SciPyorg81:online,
 author = {},
 title = {SciPy.org — SciPy.org},
 howpublished = {\url{https://www.scipy.org/}},
 month = {},
 year = {},
 note = {(Accessed on 12/15/2018)}
}

@misc{functool65:online,
 author = {},
 title = {functools — Higher-order functions and operations on callable objects — Python 3.7.2rc1 documentation},
 howpublished = {\url{https://docs.python.org/3/library/functools.html}},
 month = {},
 year = {},
 note = {(Accessed on 12/15/2018)}
}

@misc{166multi10:online,
 author = {},
 title = {16.6. multiprocessing — Process-based “threading” interface — Python 2.7.15 documentation},
 howpublished = {\url{https://docs.python.org/2/library/multiprocessing.html}},
 month = {},
 year = {},
 note = {(Accessed on 12/15/2018)}
}

@Comment{jabref-meta: databaseType:bibtex;}
